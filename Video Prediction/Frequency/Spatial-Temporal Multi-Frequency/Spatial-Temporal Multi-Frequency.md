#### Exploring Spatial-Temporal Multi-Frequency Analysis for High-Fidelity and Temporal-Consistency Video Prediction

先前视频预测方法的问题：

- 细节的损失。通常采用降采样放大接收域，提取全局信息，从而不可避免地丢失高频细节。然而，视频预测是一个像素级的密集预测问题。没有细节的帮助，就不可能做出敏锐的预测。虽然可以采用扩张卷积来避免使用下采样，但它存在网格效应的问题，对小对象不友好，阻碍了其在视频预测中的应用。
- 没有充分利用时间运动。动态场景是由具有多个时间频率的运动组成的。在图2中，左边小轿车的颞运动较低，右边大轿车的颞运动较快。它们有不同的移动频率。然而，以往的处理方法通常以固定的帧速率逐个处理。虽然递归神经网络(rnn)是用来记忆动态相关性的，但它不能区分不同频率的运动，也不能分析时间信息的时频特征。

![image-20220221204012889](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220221204012889.png)

1. 主干网络为CNN结构，在设计时采用残差密集模块
2. 为了保留更多的高频空间细节，提出了空间小波分析模块来增强高频信息表达。我们首先将输入分解为1个低频子带和3个高频子带，之后将其送入CNN进行特征提取，得到与相应主干网络通道数相同的张量，之后再将其加到主干网络上完善细节表达。
3. 为了模拟时域的多频运动，设计了时域小波分析模块。首先，先对输入序列在时间维上进行小波分析，得到一些子带，然后将这些子带连起来输入到CNN中，再将其得到对应的通道数加入到主干网络中，从而增强模型对多频运动的识别。

损失函数：

- 图像域损失：L2损失+梯度差损失：

![image-20220221205107431](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220221205107431.png)

- 对抗损失：对抗训练包括一个生成器和一个分类器，分类器学习区分图像究竟是生成或是真实的数据。通过交替训练，直到分类器无法区分图像是生成的还是真实的。

分类器上的损失为：

![image-20220221205405923](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220221205405923.png)

生成器上的损失为：

![image-20220221205421101](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220221205421101.png)

生成器的总体损失为：

![image-20220221205439909](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220221205439909.png)

代码思路很直观，就是按照论文思路实现的