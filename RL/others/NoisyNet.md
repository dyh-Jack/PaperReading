#### Noisy Networks for Exploration

探索问题在强化学习中具有重要意义，在此之前常见的探索方法有两种：

1. $\epsilon\  greedy$：智能体以概率$\epsilon$选取随机策略，否则选取当前探索到的所有action当中的最优项
2. 熵正则化(Entropy Regularization)：在standard reward上加上一个entropy regularized term来奖励随机化的策略（因为策略随机程度越高，熵就越大）

我们认为这种在行为选择上添加噪声的方法对于探索的效率较低，因此我们提出通过在神经网络的连接权重上施加参数化的噪声来提高探索效率的方法。

主要思路是通过在全连接层的权重上添加可学习的噪声来提高探索效率。在生成噪声的过程中，需要学习权重的均值$\mu$和标准差$\sigma$。如下图：其中$\epsilon$为添加的扰动的噪声。

![image-20220328182736200](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220328182736200.png)

文中引入了两种生成噪声的方法：

- 独立高斯噪声：对于每一个连接点独立从高斯分布中采样噪声，因此一共需要采样pq+q个点
- 分解高斯噪声：假设一个全连接层上层有p个神经元，下一层有q个神经元，则对于每一个神经元单独生成噪声，之后利用合成的高斯噪声作为权重的噪声。

![image-20220328182437584](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220328182437584.png)

其中$f(x) = sgn(x)\sqrt{x}$。由于分解高斯噪声对于一层只需要生成p+q个噪声，因此在实现的过程中使用分解高斯噪声。

**NoisyNet主要有两点优势：第一，在连接权重上加入噪音带来的不确定性，比在策略上加扰动要更大。第二，噪音的标准差也是学习的参数，网络通过学习可以调整噪音的大小。**
