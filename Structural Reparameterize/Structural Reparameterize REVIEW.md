## 结构重参数化REVIEW

### 定义

所谓结构重参数化，从实际操作上来说也就是：

1. 在训练时构造一系列结构（block），利用相对复杂的结构来达到提高模型精度的目的。
2. 推理时则将在训练时构造的结构进行简化，去掉一些复杂的分支，并将这些分支上在训练中得到的参数整合到主干结构中，从而在保证精确度的情况下降低模型的推理时间。

具体来说也就是，在训练时主要通过采用多分支来使模型能够学到更好更全面的特征，使得模型能够在不改变主体架构的情况下精度得到提升；而在推理时将这些分支参数通过等价变换的关系整合进入主干结构，使推理时间不受影响。

目前几种已经存在的结构重参数化的方法如下：

### ACNet

![image-20220115121640031](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220115121640031.png)

ACNet主体思路为：将每个$d\times d$的卷积核替换为三个分别为$d\times d,1\times d,d\times 1$的卷积核，构成ACNet的基本单元，在卷积计算后将这些结果线性相加即可（卷积操作满足线性相加）。而在推理阶段，将原先的$1\times d,d\times 1$卷积核直接加在$d\times d$的卷积核上，从而形成骨架增强的卷积核。

在此之前，相关工作已经证明$d\times d$的卷积核可以分解为$d\times 1,1\times d$的卷积核，但对于核的这种变换会产生显著的信息损失；因此ACNet在架构中保留了原有的卷积核，并且引入两个新的卷积核使模型的学习效果更好。

而在推理阶段，首先利用下图所示等价关系将BN层与conv层融合，之后只需要将三个分支的参数相加，合成为一个F和b，即可得到在推理阶段使用的F和b

![image-20220121200900664](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121200900664.png)

这种结构使得模型在精度上有一定的提升，而其背后的机制可能是卷积核中骨架的权重对于整体学习效果有着更为重要的印象。而通过三个卷积核的叠加，在推理时刻得到了骨架增强的卷积核，从而使得网络的准确性能够进一步提升。

### DBB

DBB与ACNet在基本思路上类似，都是利用新的架构对原有的卷积核进行重新设计，从而提高模型的精度。

不同的是，DBB在针对KxK的卷积核时，在训练阶段使用KxK，1x1+Average Pooling，1x1+KxK，1x1等四个分支来替代原有的KxK卷积核，并在推理阶段将其整合为一个KxK的卷积核。在数据集上也取得了不错的效果

![image-20220121201555821](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121201555821.png)

### ResRep

ResRep的优势我认为更多的体现在它的剪枝功能上。通过将一个KxK的卷积核在训练过程中转换为KxK+BN+1x1的卷积核，并将1x1的卷积核初始化为单位矩阵。因此1x1的卷积核并不会改变输出。

之后，需要将1x1的卷积核得到的结果转换为一个行数少于列数的矩阵

在此，引入了一种新的SGD方法

![image-20220121203209119](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121203209119.png)

由于常规SGD无法使卷积核中的参数接近于零，因此引入一个参数m，选出一部分通道将其更新的SGD公式中的m置为0，那么这一部分通道便可以在正则项的驱动下逼近于0。

由于前一项是根据损失回传的梯度，如果在KxK的卷积核上应用这种更新法则，对模型的性能将有巨大影响。因此，我们只将这种法则应用在1x1卷积核上，从而使得经过1x1卷积后的输出在一些行的数值逼近于0，而将这些参数去掉对于模型精度没有太大影响。

之后只需要再进行合并，将两个卷积核的参数合并，即可得到在推理阶段使用的卷积核参数，从而在保证性能的同时减少了模型参数

![image-20220121202318827](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121202318827.png)

### RepVGG

个人认为，RepVGG最符合我对于结构重参数化的理解。

基本思路：在训练时引入多分支，将原有的3x3卷积核扩展为3x3,1x1，identity组成的三个卷积核，从而在训练阶段获得更好的性能；

而在推理阶段，通过将三个卷积核上训练得到的数据进行整合，得到一个单通路的模型，从而能够保证推理时间不变

核心思想：

1. identity可以看做是初始化为单位矩阵的1x1卷积核（也就是1）
2. 1x1卷积核可以看做是一个极度稀疏的3x3卷积核

按照这两个思路，便可以实现卷积核的变换和整合

![image-20220121203925665](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121203925665.png)

### ResMLP

ResMLP将conv层整合到FC层中，从而在推理阶段从网络中除去了卷积操作

基本思路是进行了一系列的变换，整理如下：

![image-20220121204655827](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121204655827.png)

![image-20220121204713288](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220121204713288.png)

而在整个网络架构中共有3条通路：

1、Global Perception模块通过pooling和两个FC，从而得到一个编码全局信息的向量

2、Channel Perception模块中包含一个FC层，在此处假设o = c。传统FC有$(chw)^2$个参数，这显然不可接受。我们的解决方法是让多个通道共享一组参数，并且**禁止不同通道间的信息流**。设共有s组参数，那么每$c/s$个通道共享一组参数，此时共有$s\times (hw)^2$个参数。实际操作时，先将输入（n,c,h,w）变换为（nc/s,s,h,w），在得到输出后再反变换即可。

在实践中，Pytorch等框架并不支持参数共享，因此可以使用1x1卷积来代替：

先将$V^{(in)}(\frac{nc}{s},shw)$重塑为为$(\frac{nc}{s},shw,1,1)$；之后再对其进行s组的1x1卷积，最后将输出变换为$(\frac{nc}{s},s,h,w)$

3、Local Perception模块通过分组卷积提取局部特征，其中卷积核参数$F\in R^{s\times 1\times k \times k}$

之后将这些输出相加，即可得到最终的输出结果。

在结构重参数化的过程中，先将conv层与BN合并（和ACNet里面一模一样），之后可以直接将FC3与BN合并，最后将每一个合并后的conv_bn层按照上面的公式，将卷积核进行变换后再加到W中即可，而偏置b则可使用所有conv_bn层最终的等价b

![image-20220119143705041](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220119143705041.png)

![image-20220119184709706](C:\Users\dyh20200207\AppData\Roaming\Typora\typora-user-images\image-20220119184709706.png)